{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdVicPpG0Dzg"
   },
   "source": [
    "![picture](https://drive.google.com/uc?export=view&id=1eCsjNAtjXuXfqBLxeEnsBpOikUO06msr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEBq_OgYsOWw"
   },
   "source": [
    "# **NLP701@MBZUAI Fall 2025 - Lab 10**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9R2zbvc4E1IZ"
   },
   "source": [
    "## **Learning Outcomes**\n",
    "- Critically analyze, evaluate, and improve the performance of RNN and LSTM.\n",
    "- Gain proficiency in implementing neural networks.\n",
    "\n",
    "## **Learning Activities**\n",
    "- Implement an NER tagger using RNNs.\n",
    "- Improving the model by adding character level information or other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qngg4p84FISu"
   },
   "source": [
    "\n",
    "# **RNNs for Sequence Labeling**\n",
    "In this lab exercise, we build an NER tagger for Arabic using **RNNs**.\n",
    "We use the same dataset as assignment 1, i.e., ANERcorp that has 150K words annotated for four entities: Location (LOC), Organization (ORG), and Person (PER), and Miscellaneous (MISC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMbh0LIba68N"
   },
   "source": [
    "## **Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Xl4s-dy9hWcZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/quang.nguyen/miniconda3/lib/python3.13/site-packages (1.7.2)\n",
      "Collecting seqeval\n",
      "  Using cached seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.22.0 in /home/quang.nguyen/miniconda3/lib/python3.13/site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/quang.nguyen/miniconda3/lib/python3.13/site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/quang.nguyen/miniconda3/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/quang.nguyen/miniconda3/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "\u001b[33m  DEPRECATION: Building 'seqeval' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'seqeval'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16249 sha256=ad6f6ef059e929fecff93e4b453c477f43c85c6634325185cfa5730aef6c607e\n",
      "  Stored in directory: /home/quang.nguyen/.cache/pip/wheels/14/cf/a7/8f28ef376d707ff10e3922899482a2f23ef3002f8a952f47ac\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xEHp6iOURwR3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-23 15:38:47--  https://camel.abudhabi.nyu.edu/anercorp/ANERcorp-CamelLabSplits.zip\n",
      "Resolving camel.abudhabi.nyu.edu (camel.abudhabi.nyu.edu)... 91.230.41.24\n",
      "Connecting to camel.abudhabi.nyu.edu (camel.abudhabi.nyu.edu)|91.230.41.24|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 926414 (905K) [application/zip]\n",
      "Saving to: ‘ANERcorp-CamelLabSplits.zip’\n",
      "\n",
      "ANERcorp-CamelLabSp 100%[===================>] 904.70K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-10-23 15:38:48 (7.58 MB/s) - ‘ANERcorp-CamelLabSplits.zip’ saved [926414/926414]\n",
      "\n",
      "Archive:  ANERcorp-CamelLabSplits.zip\n",
      "   creating: ANERcorp-CamelLabSplits/\n",
      "  inflating: __MACOSX/._ANERcorp-CamelLabSplits  \n",
      "  inflating: ANERcorp-CamelLabSplits/ANERCorp_Benajiba.txt  \n",
      "  inflating: __MACOSX/ANERcorp-CamelLabSplits/._ANERCorp_Benajiba.txt  \n",
      "  inflating: ANERcorp-CamelLabSplits/ANERCorp_CamelLab_train.txt  \n",
      "  inflating: __MACOSX/ANERcorp-CamelLabSplits/._ANERCorp_CamelLab_train.txt  \n",
      "  inflating: ANERcorp-CamelLabSplits/README.txt  \n",
      "  inflating: __MACOSX/ANERcorp-CamelLabSplits/._README.txt  \n",
      "  inflating: ANERcorp-CamelLabSplits/ANERCorp_CamelLab_test.txt  \n",
      "  inflating: __MACOSX/ANERcorp-CamelLabSplits/._ANERCorp_CamelLab_test.txt  \n"
     ]
    }
   ],
   "source": [
    "!wget \"https://camel.abudhabi.nyu.edu/anercorp/ANERcorp-CamelLabSplits.zip\"\n",
    "!unzip ANERcorp-CamelLabSplits.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iZZWATjDaOvn"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# read ANER named entity dataset\n",
    "def read_data(file_path):\n",
    "    with open(file_path, mode='r') as f:\n",
    "        sent, sents = [], []\n",
    "        for line in f.readlines():\n",
    "            if len(line.strip()) > 0:\n",
    "                # split the line by space\n",
    "                token = line.strip().split(' ')\n",
    "                # unpack the token\n",
    "                word, ner = token\n",
    "                # append the tuple (word, ner tag) to a list for one sentence\n",
    "                sent.append((word, ner.replace('PERS', 'PER')))\n",
    "            # if the line is empty and the list is not empty\n",
    "            elif len(sent):\n",
    "                sents.append(sent)\n",
    "                sent = []\n",
    "        if sent:\n",
    "            sents.append(sent)\n",
    "    return sents\n",
    "\n",
    "# read training and test data\n",
    "_train_sents = read_data('./ANERcorp-CamelLabSplits/ANERCorp_CamelLab_train.txt')\n",
    "test_sents = read_data('./ANERcorp-CamelLabSplits/ANERCorp_CamelLab_test.txt')\n",
    "\n",
    "# since this dataset does not have dev set, we take 10% of the train as dev set\n",
    "train_sents, dev_sents = train_test_split(_train_sents, test_size=0.1, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "M1gFm63HSnzB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train_sents: 3575\n",
      "Size of dev_sents: 398\n",
      "The training data looks like:\n",
      "[('وقال', 'O'), ('إن', 'O'), ('العملية', 'O'), ('استهدفت', 'O'), ('\"', 'O'), ('بنى', 'O'), ('تحتية', 'O'), ('تستعمل', 'O'), ('لتخزين', 'O'), ('أسلحة', 'O'), ('عائدة', 'O'), ('للجهاد', 'O'), ('الإسلامي', 'O'), ('\"', 'O'), ('،', 'O'), ('واوضح', 'O'), ('إن', 'O'), ('إسرائيل', 'B-LOC'), ('أخطرت', 'O'), ('سكان', 'O'), ('المنطقة', 'O'), ('بوقوع', 'O'), ('الغارة', 'O'), ('.', 'O')]\n",
      "[('وهذه', 'O'), ('هي', 'O'), ('المباراة', 'O'), ('الرسمية', 'O'), ('الأولى', 'O'), ('لمنتخب', 'O'), ('إنجلترا', 'B-LOC'), ('تحت', 'O'), ('قيادة', 'O'), ('مدربه', 'O'), ('ستيف', 'B-PER'), ('مكلارين', 'I-PER'), ('الذي', 'O'), ('خلف', 'O'), ('السويدي', 'O'), ('غوران', 'B-PER'), ('إريكسون', 'I-PER'), ('بعد', 'O'), ('نهائيات', 'O'), ('المونديال', 'B-MISC'), ('.', 'O')]\n",
      "[('في', 'O'), ('عيد', 'O'), ('الملكة', 'O'), ('السماوية', 'O'), ('\"', 'O'), ('تيني', 'B-PER'), ('هاو', 'I-PER'), ('\"', 'O'), ('يسود', 'O'), ('إحساس', 'O'), ('بالروحانية', 'O'), ('ويختلط', 'O'), ('دخان', 'O'), ('البخور', 'O'), ('بالصور', 'O'), ('والأشباح', 'O'), ('.', 'O')]\n",
      "[('وأرضية', 'O'), ('المسجد', 'O'), ('ليست', 'O'), ('مفروشة', 'O'), ('بالسجاد', 'O'), ('أو', 'O'), ('الحصير', 'O'), ('وإنما', 'O'), ('هي', 'O'), ('من', 'O'), ('الرخام', 'O'), ('الأبيض', 'O'), ('فقط', 'O'), ('.', 'O')]\n",
      "[('ولن', 'O'), ('يقل', 'O'), ('سعر', 'O'), ('هذه', 'O'), ('السيارة', 'O'), ('عن', 'O'), ('6ر1', 'O'), ('مليون', 'O'), ('يورو', 'B-MISC'), ('(', 'O'), ('حوالي', 'O'), ('مليوني', 'O'), ('دولار', 'B-MISC'), (')', 'O'), ('!', 'O'), ('وتهدف', 'O'), ('شركة', 'O'), ('ديمورا', 'B-PER'), ('إلي', 'O'), ('بيعها', 'O'), ('إلي', 'O'), ('أغني', 'O'), ('أغنياء', 'O'), ('العالم', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# check the dataset\n",
    "print('Size of train_sents: %d'%len(train_sents))\n",
    "print('Size of dev_sents: %d'%len(dev_sents))\n",
    "\n",
    "print('The training data looks like:')\n",
    "for sent in train_sents[:5]:\n",
    "  print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIgBZ9ZoEEBy"
   },
   "source": [
    "## **Build Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pD4wQSwW43xt"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "def build_vocab(sents, idx, special_tokens=['UNK']):\n",
    "    vocab = {}\n",
    "    if special_tokens:\n",
    "        vocab = {k: v for v, k in enumerate(special_tokens)}\n",
    "\n",
    "    for sent in sents:\n",
    "        for word in sent:\n",
    "            if word[idx] not in vocab:\n",
    "                vocab[word[idx]] = len(vocab)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "def prepare_sequence(seq, to_idx):\n",
    "    idxs = []\n",
    "    for w in seq:\n",
    "        if w in to_idx:\n",
    "            idxs.append(to_idx[w])\n",
    "        else:\n",
    "            idxs.append(to_idx['UNK'])\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IH89pNcDOOj3"
   },
   "outputs": [],
   "source": [
    "word_to_idx = build_vocab(train_sents, 0)\n",
    "tag_to_idx = build_vocab(train_sents, 1, special_tokens=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDt1doaxEEBz"
   },
   "source": [
    "## **Create the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nnePUgCPEEBz"
   },
   "outputs": [],
   "source": [
    "class RNNTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(RNNTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Word embedding layer. This maps each word to a vector representation.\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The RNN takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        # You can utilize the multi-layer rnn by changing num_layers.\n",
    "        # Also you can use the bidirectional rnn with bidirectional=True.\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=1, bidirectional=False)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        rnn_out, _ = self.rnn(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(rnn_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        with torch.no_grad():\n",
    "            inputs = prepare_sequence(sentence, word_to_idx)\n",
    "            tag_scores = self.forward(inputs)\n",
    "            _, indices = torch.max(tag_scores, 1)\n",
    "            tags = []\n",
    "            for i in range(len(indices)):\n",
    "                for key, value in tag_to_idx.items():\n",
    "                    if indices[i] == value:\n",
    "                        tags.append(key)\n",
    "        return tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9wsoOAhGdrd"
   },
   "source": [
    "## **Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XPB8U1eBTVGB"
   },
   "outputs": [],
   "source": [
    "def train(model, train_sents, lr=0.1, epochs=3):\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for sentence in tqdm(train_sents):\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            optimizer.zero_grad() # or model.zero_grad()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            x = [word[0] for word in sentence]\n",
    "            y = [word[1] for word in sentence]\n",
    "            sentence_in = prepare_sequence(x, word_to_idx)\n",
    "            targets = prepare_sequence(y, tag_to_idx)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model(sentence_in)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'epoch: {epoch+1}, loss: {loss}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1BqE6Ul_EEB0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3575/3575 [00:09<00:00, 358.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.1423053741455078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3575/3575 [00:09<00:00, 375.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, loss: 0.0937434509396553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3575/3575 [00:10<00:00, 340.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, loss: 0.055363357067108154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3575/3575 [00:10<00:00, 337.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, loss: 0.03216924890875816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3575/3575 [00:10<00:00, 328.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, loss: 0.029427779838442802\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "RNN_model = RNNTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_idx), len(tag_to_idx))\n",
    "RNN_tagger = train(RNN_model, train_sents, epochs=5)\n",
    "\n",
    "torch.save(RNN_tagger.state_dict(), './rnn_tagger.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAHwV2n_HNbx"
   },
   "source": [
    "## **Evaluate the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEMLv0TmLNcR"
   },
   "source": [
    "Load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "C8s4FhAuLLBl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNTagger(\n",
       "  (word_embeddings): Embedding(27364, 128)\n",
       "  (rnn): RNN(128, 256)\n",
       "  (hidden2tag): Linear(in_features=256, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNN_tagger = RNNTagger(EMBEDDING_DIM,\n",
    "                       HIDDEN_DIM,\n",
    "                       len(word_to_idx),\n",
    "                       len(tag_to_idx))\n",
    "RNN_tagger.load_state_dict(torch.load('./rnn_tagger.pt'))\n",
    "RNN_tagger.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHWMJa7YLNBq"
   },
   "source": [
    "Evalute the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aYGoar04HUHx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.44234404536862004)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "dev = [[word[0] for word in sent] for sent in dev_sents]\n",
    "dev_gold = [[word[1] for word in sent] for sent in dev_sents]\n",
    "\n",
    "dev_pred = [RNN_tagger.predict(sent) for sent in dev]\n",
    "f1_score(dev_gold, dev_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLLm1yxEGpln"
   },
   "source": [
    "# **Exercise 01: LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44ky3bpzNcNY"
   },
   "source": [
    "Create the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msWXgsp9TOm8"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        with torch.no_grad():\n",
    "            inputs = prepare_sequence(sentence, word_to_idx)\n",
    "            tag_scores = self.forward(inputs)\n",
    "            _, indices = torch.max(tag_scores, 1)\n",
    "            tags = []\n",
    "            for i in range(len(indices)):\n",
    "                for key, value in tag_to_idx.items():\n",
    "                    if indices[i] == value:\n",
    "                        tags.append(key)\n",
    "        return tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F66ViF92Ndv7"
   },
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljz3kg4HEEB1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3575/3575 [00:09<00:00, 377.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.20392757654190063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3575/3575 [00:09<00:00, 372.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, loss: 0.07214045524597168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3575/3575 [00:09<00:00, 365.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, loss: 0.023382777348160744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3575/3575 [00:09<00:00, 362.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, loss: 0.013818600215017796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3575/3575 [00:09<00:00, 363.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, loss: 0.019663896411657333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "\"\"\"\n",
    "[Your code here]\n",
    "\"\"\"\n",
    "\n",
    "lstm_model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_idx), len(tag_to_idx))\n",
    "lstm_tagger = train(lstm_model, train_sents, epochs=5)\n",
    "\n",
    "torch.save(lstm_tagger.state_dict(), './lstm_tagger.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eo1KDLMBNgBp"
   },
   "source": [
    "Evalute the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "AYtIblJ8Mhom"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5803971812940423)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "[Your code here]\n",
    "\"\"\"\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "dev = [[word[0] for word in sent] for sent in dev_sents]\n",
    "dev_gold = [[word[1] for word in sent] for sent in dev_sents]\n",
    "\n",
    "dev_pred = [lstm_tagger.predict(sent) for sent in dev]\n",
    "f1_score(dev_gold, dev_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OEiEeOYUEHm"
   },
   "source": [
    "## **What would be a good example to show that LSTM performs better than a simple RNN?**\n",
    "- No coding involved, and no need to use Arabic as an example.\n",
    "- Imagine you're writing a paper, and you want to show an example that works well in LSTM compared to RNN.\n",
    "- What kind of example would you present and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YW_bOlrkUHO9"
   },
   "source": [
    "### Example that requires Long-Term Dependency like: \n",
    "\n",
    "The movie started off dull, with weak performances and predictable dialogue, but by the end, it delivered an incredibly emotional and satisfying conclusion.\n",
    "- Simple RNN: early information (“dull”, “weak performances”) gets overwritten by later inputs. By the time the model reaches “emotional and satisfying conclusion,” it has largely forgotten the earlier context that helps interpret the reversal of sentiment.\n",
    "- LSTM: can preserve earlier important cues (like “but by the end”) and decide what to forget or remember. It learns that “but by the end” signals a contrast — the final sentiment depends more on the later clause than the initial one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUvta4DPM6xH"
   },
   "source": [
    "# **Bonus Exercise: How Would You Improve the Model?**\n",
    "- What can be done on top of the simple LSTM tagger to improve the performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "oszrGTZxVC54"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3575 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3575/3575 [00:23<00:00, 153.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.09997943043708801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3575/3575 [00:22<00:00, 159.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, loss: 0.18903404474258423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3575/3575 [00:21<00:00, 162.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, loss: 0.3200090229511261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3575/3575 [00:21<00:00, 163.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, loss: 0.021573113277554512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3575/3575 [00:21<00:00, 165.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, loss: 0.05264861881732941\n"
     ]
    }
   ],
   "source": [
    "class LSTMImproveTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMImproveTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        with torch.no_grad():\n",
    "            inputs = prepare_sequence(sentence, word_to_idx)\n",
    "            tag_scores = self.forward(inputs)\n",
    "            _, indices = torch.max(tag_scores, 1)\n",
    "            tags = []\n",
    "            for i in range(len(indices)):\n",
    "                for key, value in tag_to_idx.items():\n",
    "                    if indices[i] == value:\n",
    "                        tags.append(key)\n",
    "        return tags\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "def train(model, train_sents, lr=0.1, epochs=3):\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for sentence in tqdm(train_sents):\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            optimizer.zero_grad() # or model.zero_grad()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            x = [word[0] for word in sentence]\n",
    "            y = [word[1] for word in sentence]\n",
    "            sentence_in = prepare_sequence(x, word_to_idx)\n",
    "            targets = prepare_sequence(y, tag_to_idx)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model(sentence_in)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(f'epoch: {epoch+1}, loss: {loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "improvebilstm_model = LSTMImproveTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_idx), len(tag_to_idx))\n",
    "improvebilstm_tagger = train(improvebilstm_model, train_sents, lr=0.01, epochs=5)\n",
    "\n",
    "torch.save(improvebilstm_model.state_dict(), './lstm_improved_tagger.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6088362068965518)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "dev = [[word[0] for word in sent] for sent in dev_sents]\n",
    "dev_gold = [[word[1] for word in sent] for sent in dev_sents]\n",
    "\n",
    "dev_pred = [improvebilstm_tagger.predict(sent) for sent in dev]\n",
    "f1_score(dev_gold, dev_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use bidirectional LSTM to improve performance => F1: 60.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBKgcQ_nEEB2"
   },
   "source": [
    "## **Reference**\n",
    "https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
